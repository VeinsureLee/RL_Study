{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5c283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.argv = ['']\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.grid_world import GridWorld\n",
    "from examples.agent import Agent\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e87645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Q(s, a) 网络\n",
    "    输入: 状态one-hot向量（num_states维）\n",
    "    输出: 所有动作的Q值（num_actions维）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_states, num_actions, emb_dim = 8):\n",
    "        super(QNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_states, emb_dim)\n",
    "        self.fc1 = nn.Linear(emb_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1b4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62479c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, env, num_episodes=5000, episode_length=1000,\n",
    "                 gamma=0.9, alpha=1e-2, epsilon=0.1,\n",
    "                 update_freq=100, mini_batch_size=64):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            num_episodes=num_episodes,\n",
    "            episode_length=episode_length\n",
    "        )\n",
    "\n",
    "        self.alpha = alpha\n",
    "        # 初始化Q网络与目标网络\n",
    "        self.QNet = QNet(self.num_states, self.num_actions)\n",
    "        self.target_QNet = QNet(self.num_states, self.num_actions)\n",
    "        self.target_QNet.load_state_dict(self.QNet.state_dict())\n",
    "\n",
    "        # 对应策略\n",
    "        self.policy = np.ones((self.num_states, self.num_actions)) / self.num_actions\n",
    "        self.target_policy = np.ones((self.num_states, self.num_actions)) / self.num_actions\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.QNet.parameters(), lr=alpha)\n",
    "\n",
    "        self.update_freq = update_freq\n",
    "        self.buffer = ReplayBuffer(capacity=episode_length*num_episodes)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.step_counter = 0  # 用于定期更新目标网络\n",
    "\n",
    "    def run(self):\n",
    "        episodes_per_iteration = int(self.num_episodes / 10)\n",
    "        for iteration in range(10):\n",
    "            progress_desc = f\"Iteration {iteration + 1}/10\"\n",
    "            with tqdm(total=episodes_per_iteration, desc=progress_desc, unit='episode') as pbar:\n",
    "                for episode_idx in range(episodes_per_iteration):\n",
    "                    global_episode = iteration * episodes_per_iteration + episode_idx + 1\n",
    "\n",
    "                    self.env.reset()\n",
    "                    current_state = self.env.start_state\n",
    "                    current_state_idx = self.state2idx(current_state)\n",
    "                    current_action_idx = self.choose_action(current_state_idx)\n",
    "\n",
    "                    episode_reward = 0\n",
    "                    step_count = 0\n",
    "\n",
    "                    while step_count < self.episode_length:\n",
    "                        current_action = self.idx2action(current_action_idx)\n",
    "                        next_state, reward, done, _ = self.env.step(current_action)\n",
    "                        next_state_idx = self.state2idx(next_state)\n",
    "                        next_action_idx = self.choose_action(next_state_idx)\n",
    "\n",
    "                        episode_reward += reward\n",
    "\n",
    "                        # 存入经验\n",
    "                        self.buffer.add(current_state_idx, current_action_idx, reward, next_state_idx, done)\n",
    "\n",
    "                        # 更新Q网络与策略\n",
    "                        if self.buffer.size() >= self.mini_batch_size:\n",
    "                            self.update()\n",
    "                            self.update_Q()\n",
    "\n",
    "                        # 更新当前状态和动作\n",
    "                        current_state_idx = next_state_idx\n",
    "                        current_action_idx = next_action_idx\n",
    "                        step_count += 1\n",
    "                        self.step_counter += 1\n",
    "\n",
    "                        # 每 update_freq 步同步一次目标网络和 target_policy\n",
    "                        if self.step_counter % self.update_freq == 0:\n",
    "                            self.update_target_policy()\n",
    "\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                    if global_episode % 10 == 0:\n",
    "                        pbar.set_postfix({\n",
    "                            'episode': f\"{global_episode}\",\n",
    "                            'return': f\"{episode_reward:.3f}\"\n",
    "                        })\n",
    "                    pbar.update(1)\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        从ReplayBuffer采样批量经验，更新QNet及policy\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.mini_batch_size)\n",
    "\n",
    "        states_tensor = F.one_hot(torch.tensor(states, dtype=torch.long), num_classes=self.num_states)\n",
    "        next_states_tensor = F.one_hot(torch.tensor(next_states, dtype=torch.long), num_classes=self.num_states)\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float)\n",
    "\n",
    "        # 当前Q值\n",
    "        q_values = self.QNet(states_tensor)\n",
    "        \n",
    "        if q_values.dim() == 2:\n",
    "            # 标准情况：batch_size > 1\n",
    "            q_values = q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            # 如果q_values是1D（batch_size=1的特殊情况）\n",
    "            q_values = q_values.gather(0, actions_tensor)\n",
    "\n",
    "        # 目标Q值\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_QNet(next_states_tensor)\n",
    "            max_next_q_values, _ = next_q_values.max(dim=1)\n",
    "            td_targets = rewards_tensor + self.gamma * max_next_q_values * (1 - dones_tensor)\n",
    "\n",
    "        # 损失与优化\n",
    "        loss = self.loss_fn(q_values, td_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 更新对应的 policy\n",
    "        for idx in range(self.mini_batch_size):\n",
    "            self.update_policy(states[idx])\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_policy(self, st_idx):\n",
    "        \"\"\"\n",
    "        更新 QNet 对应的 policy\n",
    "        \"\"\"\n",
    "        st_tensor = F.one_hot(torch.tensor(st_idx, dtype=torch.long), num_classes=self.num_states).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.QNet(st_tensor).squeeze(0).numpy()\n",
    "        greedy_action = np.argmax(q_values)\n",
    "        for a in range(self.num_actions):\n",
    "            if a == greedy_action:\n",
    "                self.policy[st_idx, a] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "            else:\n",
    "                self.policy[st_idx, a] = self.epsilon / self.num_actions\n",
    "\n",
    "    def update_target_policy(self):\n",
    "        \"\"\"\n",
    "        更新目标网络和 target_policy\n",
    "        \"\"\"\n",
    "        self.target_QNet.load_state_dict(self.QNet.state_dict())\n",
    "        for st_idx in range(self.num_states):\n",
    "            st_tensor = F.one_hot(torch.tensor(st_idx, dtype=torch.long), num_classes=self.num_states).float().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.target_QNet(st_tensor).squeeze(0).numpy()\n",
    "            greedy_action = np.argmax(q_values)\n",
    "            for a in range(self.num_actions):\n",
    "                if a == greedy_action:\n",
    "                    self.target_policy[st_idx, a] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "                else:\n",
    "                    self.target_policy[st_idx, a] = self.epsilon / self.num_actions\n",
    "\n",
    "    def update_Q(self):\n",
    "        for st_idx in range(self.num_states):\n",
    "            st_tensor = F.one_hot(torch.tensor(st_idx, dtype=torch.long), num_classes=self.num_states).float().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.QNet(st_tensor).squeeze(0).numpy()\n",
    "            self.Q[st_idx, :] = q_values\n",
    "        return self.Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ed7de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "env.reward_step = -1\n",
    "env.reward_target = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90eb99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1/10:   0%|          | 0/100 [00:00<?, ?episode/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN(env, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m agent\u001b[38;5;241m.\u001b[39mrender_static()\n",
      "Cell \u001b[1;32mIn[19], line 60\u001b[0m, in \u001b[0;36mDQN.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 更新Q网络与策略\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_batch_size:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_Q()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 更新当前状态和动作\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 103\u001b[0m, in \u001b[0;36mDQN.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# 如果q_values是1D（batch_size=1的特殊情况）\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# 目标Q值\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "agent = DQN(env, epsilon=0.3, gamma=1, num_episodes=1000, episode_length=1000, alpha=1e-3)\n",
    "agent.run()\n",
    "agent.render_static()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61f1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
