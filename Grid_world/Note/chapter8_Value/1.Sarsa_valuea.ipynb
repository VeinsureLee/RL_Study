{
 "cells": [
  {
   "cell_type": "code",
   "id": "2a35bebe5e8d361a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.344486Z",
     "start_time": "2026-01-10T10:06:30.340706Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = ['']\n",
    "sys.path.append(\"../..\")\n",
    "from src.grid_world import GridWorld\n",
    "from examples.agent import Agent\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q Network\n",
    "\n",
    "输入：状态（二维，归一化），动作（二维，dx，dy）\n",
    "\n",
    "输出：Q值"
   ],
   "id": "7de4c8f796bea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.374262Z",
     "start_time": "2026-01-10T10:06:30.364396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, states_dim, actions_num, hidden_dim=128):\n",
    "        super(QNet, self).__init__()\n",
    "        # states_dim: 状态维度,normalized后维度\n",
    "        # actions_num: 动作维度,one-hot编码后维度\n",
    "        self.fc1 = nn.Linear(states_dim + actions_num, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "ac0e842e13df7d38",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 经验回放缓冲区\n",
   "id": "d59afdf1040f929c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.398692Z",
     "start_time": "2026-01-10T10:06:30.388526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, next_action, done):\n",
    "        self.buffer.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, next_actions, dones = zip(*transitions)\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(next_actions),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()"
   ],
   "id": "4e5e502217fee3f1",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 测试经验回放缓冲区",
   "id": "461567bfc0dbcbf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.471934Z",
     "start_time": "2026-01-10T10:06:30.462482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test cell\n",
    "buffer = ReplayBuffer(capacity=100)\n",
    "buffer.add((0,0), (0, 1), 10, (0,1), (0, 1), False)\n",
    "buffer.add((0,1), (1, 1), 10, (1,1), (1, 1), False)\n",
    "print(buffer.sample(1))\n",
    "buffer.clear()\n",
    "print(buffer.size())"
   ],
   "id": "35e1f989dbcebcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 1]]), array([[1, 1]]), array([10]), array([[1, 1]]), array([[1, 1]]), array([False]))\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sarsa Value Agent",
   "id": "2731571c42b8c9f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.507992Z",
     "start_time": "2026-01-10T10:06:30.489743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SarsaValue(Agent):\n",
    "    def __init__(self,\n",
    "                 env, epsilon=0.1, gamma=0.99, alpha=0.1,\n",
    "                 num_episodes=10, episode_length=1024,\n",
    "                 batch_size=256, state_dim=2, action_dim=2,\n",
    "                 num_epochs=10\n",
    "                 ):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            num_episodes=num_episodes,\n",
    "            episode_length=episode_length\n",
    "        )\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_length = episode_length\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.x_col = int(env.env_size[0])\n",
    "        self.y_row = int(env.env_size[1])\n",
    "        self.num_actions = env.num_actions\n",
    "        self.action_space = env.action_space\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.QNet = QNet(states_dim=state_dim, actions_num=self.num_actions, hidden_dim=128)\n",
    "        self.optimizer = optim.Adam(self.QNet.parameters(), lr=self.alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.V = np.zeros((self.x_col, self.y_row))\n",
    "        self.Q = np.zeros((self.x_col, self.y_row, self.num_actions))\n",
    "        self.policy = np.ones((self.x_col, self.y_row, self.num_actions)) / self.num_actions\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = ReplayBuffer(capacity=episode_length*num_episodes)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        Take an action according to current policy\n",
    "        \"\"\"\n",
    "        x, y = state  # state = (x, y)\n",
    "        probs = self.policy[x, y]\n",
    "        action_idx = np.random.choice(np.arange(self.num_actions), p=probs)\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def generate_episode(self):\n",
    "        self.env.reset()\n",
    "        s = self.env.start_state\n",
    "        a = self.take_action(s)\n",
    "        episodes_list = []\n",
    "        for t in range(self.episode_length):\n",
    "            s_next, reward, done, _ = self.env.step(a)\n",
    "            a_next = self.take_action(s_next)\n",
    "            if (s, a, s_next, reward, done) not in episodes_list:\n",
    "                episodes_list.append((s, a, s_next, reward, done))\n",
    "                self.buffer.add(s, a, reward, s_next, a_next, done)\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "            if done:\n",
    "                break\n",
    "        return episodes_list\n",
    "\n",
    "    def action2onehot(self, actions):\n",
    "        \"\"\"\n",
    "        将动作转换为one-hot编码形式\n",
    "        :param actions: (dx, dy), ...\n",
    "        :return one-hot\n",
    "        \"\"\"\n",
    "        dct = {action: idx for idx, action in enumerate(self.action_space)}\n",
    "        indices = [dct[tuple(action)] for action in actions]\n",
    "        one_hot = np.eye(self.num_actions)[indices]\n",
    "        return one_hot\n",
    "\n",
    "    def state_action_to_tensor(self, states, actions):\n",
    "        \"\"\"\n",
    "        将状态和动作转换为张量形式,并将states归一, 动作one-hot编码\n",
    "        :param states: (x, y), ...\n",
    "        :param actions: (dx, dy), ...\n",
    "        :return 拼接后的tensors，可以直接输入QNet\n",
    "        \"\"\"\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        states[:, 0] = states[:, 0] / (self.x_col - 1)\n",
    "        states[:, 1] = states[:, 1] / (self.y_row - 1)\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float)\n",
    "\n",
    "        actions_onehot = self.action2onehot(actions)   # (B, num_actions)\n",
    "        actions_tensor = torch.tensor(actions_onehot, dtype=torch.float)\n",
    "\n",
    "        state_action_tensor = torch.cat([states_tensor, actions_tensor], dim=1)\n",
    "        return state_action_tensor\n",
    "\n",
    "    def update_action_value(self):\n",
    "        \"\"\"\n",
    "        使用当前QNet对整个状态-动作空间进行估计，并赋值给self.Q\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                for a in self.action_space:\n",
    "                    state_action = self.state_action_to_tensor([(x, y)], [a])\n",
    "                    with torch.no_grad():\n",
    "                        q_value = self.QNet(state_action).item()\n",
    "                    self.Q[x, y, self.action2idx(a)] = q_value\n",
    "        return self.Q\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        根据当前self.Q估计ε-greedy策略，并赋值给self.policy\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                best_a = np.argmax(self.Q[x, y])\n",
    "                for a in range(self.num_actions):\n",
    "                    if a == best_a:\n",
    "                        self.policy[x, y, a] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "                    else:\n",
    "                        self.policy[x, y, a] = self.epsilon / self.num_actions\n",
    "        return self.policy\n",
    "\n",
    "    def update_state_value(self):\n",
    "        \"\"\"\n",
    "        根据当前self.Q和self.policy计算状态值V(s)，并赋值给self.V\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                self.V[x, y] = np.sum(self.policy[x, y] * self.Q[x, y])\n",
    "        return self.V\n",
    "\n",
    "    def update_QNet(self):\n",
    "        loss_avg = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if buffer.size() < self.batch_size:\n",
    "                states, actions, rewards, next_states, next_actions, dones = self.buffer.sample(self.buffer.size())\n",
    "            else:\n",
    "                states, actions, rewards, next_states, next_actions, dones = self.buffer.sample(self.batch_size)\n",
    "            # 转换为张量\n",
    "            states_actions = self.state_action_to_tensor(states, actions)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float).view(-1, 1)\n",
    "            next_states_actions = self.state_action_to_tensor(next_states, next_actions)\n",
    "            dones_tensor = torch.tensor(dones, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "            # 计算当前 Q 值\n",
    "            q_values = self.QNet(states_actions)\n",
    "\n",
    "            # 计算目标 Q 值\n",
    "            with torch.no_grad():\n",
    "                q_next_values = self.QNet(next_states_actions)\n",
    "                td_target = rewards_tensor + self.gamma * q_next_values * (1 - dones_tensor)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_fn(q_values, td_target)\n",
    "\n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "        return loss_avg/self.num_epochs\n",
    "\n",
    "    def run(self):\n",
    "        loss = 0\n",
    "        for episode in range(self.num_episodes):\n",
    "            # 清空缓冲区\n",
    "            self.buffer.clear()\n",
    "            # 生成1条episode并存入缓冲区\n",
    "            self.generate_episode()\n",
    "            # 进行多次小批量更新\n",
    "            loss += self.update_QNet()\n",
    "            # 更新Q值估计\n",
    "            self.update_action_value()\n",
    "            # 更新策略\n",
    "            self.update_policy()\n",
    "            # 更新状态值\n",
    "            self.update_state_value()\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode + 1} finished, loss: {loss/10:.4f}\")\n",
    "                loss = 0"
   ],
   "id": "98e76ff6ea0ce41a",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:06:30.543975Z",
     "start_time": "2026-01-10T10:06:30.521030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_test = GridWorld()\n",
    "env_test.reward_step = 0\n",
    "env_test.reward_target = 100\n",
    "\n",
    "agent_test = SarsaValue(env_test, epsilon=0.1, gamma=0.99, num_episodes=1000, episode_length=1000, batch_size=512, num_epochs=100, alpha=1e-3)\n",
    "\n",
    "print(len(agent_test.generate_episode()))\n",
    "\n"
   ],
   "id": "c7af6b130bedd0de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:09:32.607290Z",
     "start_time": "2026-01-10T10:09:05.359501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = GridWorld()\n",
    "env.reward_step = -1\n",
    "env.reward_target = 10\n",
    "\n",
    "agent = SarsaValue(env,\n",
    "                   epsilon=0.1, gamma=1,\n",
    "                   num_episodes=1000, episode_length=1000,\n",
    "                   batch_size=16, num_epochs=100,\n",
    "                   alpha=1e-3\n",
    "                   )\n",
    "agent.run()\n",
    "# agent.render_static()\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print(agent.get_policy())"
   ],
   "id": "6da056dc16dd0156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 finished, loss: 13.8558\n",
      "Episode 20 finished, loss: 7.4202\n",
      "Episode 30 finished, loss: 32.5152\n",
      "Episode 40 finished, loss: 14.2297\n",
      "Episode 50 finished, loss: 20.7435\n",
      "Episode 60 finished, loss: 8.8885\n",
      "Episode 70 finished, loss: 28.1663\n",
      "Episode 80 finished, loss: 54.5157\n",
      "Episode 90 finished, loss: 115.7929\n",
      "Episode 100 finished, loss: 6.6832\n",
      "Episode 110 finished, loss: 4.4667\n",
      "Episode 120 finished, loss: 18.3007\n",
      "Episode 130 finished, loss: 28.2491\n",
      "Episode 140 finished, loss: 3.5482\n",
      "Episode 150 finished, loss: 4333.6950\n",
      "Episode 160 finished, loss: 15813.3268\n",
      "Episode 170 finished, loss: 10106.9195\n",
      "Episode 180 finished, loss: 3170.5890\n",
      "Episode 190 finished, loss: 2714.1792\n",
      "Episode 200 finished, loss: 1387.0600\n",
      "Episode 210 finished, loss: 1407.4724\n",
      "Episode 220 finished, loss: 326.6655\n",
      "Episode 230 finished, loss: 500.1487\n",
      "Episode 240 finished, loss: 443.4448\n",
      "Episode 250 finished, loss: 453.7953\n",
      "Episode 260 finished, loss: 486.2608\n",
      "Episode 270 finished, loss: 610.4077\n",
      "Episode 280 finished, loss: 881.3260\n",
      "Episode 290 finished, loss: 1179.2892\n",
      "Episode 300 finished, loss: 1549.0894\n",
      "Episode 310 finished, loss: 2001.1805\n",
      "Episode 320 finished, loss: 2381.2962\n",
      "Episode 330 finished, loss: 3004.0122\n",
      "Episode 340 finished, loss: 3544.9906\n",
      "Episode 350 finished, loss: 4185.9779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 11\u001B[0m\n\u001B[0;32m      3\u001B[0m env\u001B[38;5;241m.\u001B[39mreward_target \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      5\u001B[0m agent \u001B[38;5;241m=\u001B[39m SarsaValue(env, \n\u001B[0;32m      6\u001B[0m                    epsilon\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, gamma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      7\u001B[0m                    num_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, episode_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, \n\u001B[0;32m      8\u001B[0m                    batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, \n\u001B[0;32m      9\u001B[0m                    alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m\n\u001B[0;32m     10\u001B[0m                    )\n\u001B[1;32m---> 11\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# agent.render_static()\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinal Policy:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[66], line 171\u001B[0m, in \u001B[0;36mSarsaValue.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer\u001B[38;5;241m.\u001B[39mclear()\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# 生成1条episode并存入缓冲区\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_episode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# 进行多次小批量更新\u001B[39;00m\n\u001B[0;32m    173\u001B[0m loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_QNet()\n",
      "Cell \u001B[1;32mIn[66], line 57\u001B[0m, in \u001B[0;36mSarsaValue.generate_episode\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepisode_length):\n\u001B[0;32m     56\u001B[0m     s_next, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(a)\n\u001B[1;32m---> 57\u001B[0m     a_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_next\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (s, a, s_next, reward, done) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m episodes_list:\n\u001B[0;32m     59\u001B[0m         episodes_list\u001B[38;5;241m.\u001B[39mappend((s, a, s_next, reward, done))\n",
      "Cell \u001B[1;32mIn[66], line 47\u001B[0m, in \u001B[0;36mSarsaValue.take_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     45\u001B[0m x, y \u001B[38;5;241m=\u001B[39m state  \u001B[38;5;66;03m# state = (x, y)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy[x, y]\n\u001B[1;32m---> 47\u001B[0m action_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_actions\u001B[49m\u001B[43m)\u001B[49m, p\u001B[38;5;241m=\u001B[39mprobs)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space[action_idx]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T10:07:59.088158Z",
     "start_time": "2026-01-10T10:07:59.079748Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c464b3eee4c560fb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
