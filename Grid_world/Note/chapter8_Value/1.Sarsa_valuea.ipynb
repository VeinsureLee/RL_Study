{
 "cells": [
  {
   "cell_type": "code",
   "id": "2a35bebe5e8d361a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:24.312780Z",
     "start_time": "2026-01-10T09:34:22.203205Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = ['']\n",
    "sys.path.append(\"../..\")\n",
    "from src.grid_world import GridWorld\n",
    "from examples.agent import Agent\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Q Network\n",
    "\n",
    "输入：状态（二维，归一化），动作（二维，dx，dy）\n",
    "\n",
    "输出：Q值"
   ],
   "id": "7de4c8f796bea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:24.543007Z",
     "start_time": "2026-01-10T09:34:24.526523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, states_dim, actions_num, hidden_dim=128):\n",
    "        super(QNet, self).__init__()\n",
    "        # states_dim: 状态维度,normalized后维度\n",
    "        # actions_num: 动作维度,one-hot编码后维度\n",
    "        self.fc1 = nn.Linear(states_dim + actions_num, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "ac0e842e13df7d38",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 经验回放缓冲区\n",
   "id": "d59afdf1040f929c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:24.561251Z",
     "start_time": "2026-01-10T09:34:24.555247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, next_action, done):\n",
    "        self.buffer.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, next_actions, dones = zip(*transitions)\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(next_actions),\n",
    "            np.array(dones)\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()"
   ],
   "id": "4e5e502217fee3f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 测试经验回放缓冲区",
   "id": "461567bfc0dbcbf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:24.591571Z",
     "start_time": "2026-01-10T09:34:24.584239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test cell\n",
    "buffer = ReplayBuffer(capacity=100)\n",
    "buffer.add((0,0), (0, 1), 10, (0,1), (0, 1), False)\n",
    "buffer.add((0,1), (1, 1), 10, (1,1), (1, 1), False)\n",
    "print(buffer.sample(1))\n",
    "buffer.clear()\n",
    "print(buffer.size())"
   ],
   "id": "35e1f989dbcebcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 1]]), array([[1, 1]]), array([10]), array([[1, 1]]), array([[1, 1]]), array([False]))\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sarsa Value Agent",
   "id": "2731571c42b8c9f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:24.633998Z",
     "start_time": "2026-01-10T09:34:24.622896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SarsaValue(Agent):\n",
    "    def __init__(self,\n",
    "                 env, epsilon=0.1, gamma=0.99, alpha=0.1,\n",
    "                 num_episodes=10, episode_length=1024,\n",
    "                 batch_size=256, state_dim=2, action_dim=2,\n",
    "                 num_epochs=10\n",
    "                 ):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            epsilon=epsilon,\n",
    "            gamma=gamma,\n",
    "            num_episodes=num_episodes,\n",
    "            episode_length=episode_length\n",
    "        )\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_length = episode_length\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.x_col = int(env.env_size[0])\n",
    "        self.y_row = int(env.env_size[1])\n",
    "        self.num_actions = env.num_actions\n",
    "        self.action_space = env.action_space\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.QNet = QNet(states_dim=state_dim, actions_num=self.num_actions, hidden_dim=128)\n",
    "        self.optimizer = optim.Adam(self.QNet.parameters(), lr=self.alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.V = np.zeros((self.x_col, self.y_row))\n",
    "        self.Q = np.zeros((self.x_col, self.y_row, self.num_actions))\n",
    "        self.policy = np.ones((self.x_col, self.y_row, self.num_actions)) / self.num_actions\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = ReplayBuffer(capacity=episode_length*num_episodes)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        Take an action according to current policy\n",
    "        \"\"\"\n",
    "        x, y = state  # state = (x, y)\n",
    "        probs = self.policy[x, y]\n",
    "        action_idx = np.random.choice(np.arange(self.num_actions), p=probs)\n",
    "        return self.action_space[action_idx]\n",
    "\n",
    "    def generate_episode(self):\n",
    "        self.env.reset()\n",
    "        s = self.env.start_state\n",
    "        a = self.take_action(s)\n",
    "        for t in range(self.episode_length):\n",
    "            s_next, reward, done, _ = self.env.step(a)\n",
    "            a_next = self.take_action(s_next)\n",
    "            self.buffer.add(s, a, reward, s_next, a_next, done)\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "            if done:\n",
    "                break\n",
    "        return self.buffer\n",
    "\n",
    "    def action2onehot(self, actions):\n",
    "        \"\"\"\n",
    "        将动作转换为one-hot编码形式\n",
    "        :param actions: (dx, dy), ...\n",
    "        :return one-hot\n",
    "        \"\"\"\n",
    "        dct = {action: idx for idx, action in enumerate(self.action_space)}\n",
    "        indices = [dct[tuple(action)] for action in actions]\n",
    "        one_hot = np.eye(self.num_actions)[indices]\n",
    "        return one_hot\n",
    "\n",
    "    def state_action_to_tensor(self, states, actions):\n",
    "        \"\"\"\n",
    "        将状态和动作转换为张量形式,并将states归一, 动作one-hot编码\n",
    "        :param states: (x, y), ...\n",
    "        :param actions: (dx, dy), ...\n",
    "        :return 拼接后的tensors，可以直接输入QNet\n",
    "        \"\"\"\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        states[:, 0] = states[:, 0] / (self.x_col - 1)\n",
    "        states[:, 1] = states[:, 1] / (self.y_row - 1)\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float)\n",
    "\n",
    "        actions_onehot = self.action2onehot(actions)   # (B, num_actions)\n",
    "        actions_tensor = torch.tensor(actions_onehot, dtype=torch.float)\n",
    "\n",
    "        state_action_tensor = torch.cat([states_tensor, actions_tensor], dim=1)\n",
    "        return state_action_tensor\n",
    "\n",
    "    def update_action_value(self):\n",
    "        \"\"\"\n",
    "        使用当前QNet对整个状态-动作空间进行估计，并赋值给self.Q\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                for a in self.action_space:\n",
    "                    state_action = self.state_action_to_tensor([(x, y)], [a])\n",
    "                    with torch.no_grad():\n",
    "                        q_value = self.QNet(state_action).item()\n",
    "                    self.Q[x, y, self.action2idx(a)] = q_value\n",
    "        return self.Q\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"\n",
    "        根据当前self.Q估计ε-greedy策略，并赋值给self.policy\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                best_a = np.argmax(self.Q[x, y])\n",
    "                for a in range(self.num_actions):\n",
    "                    if a == best_a:\n",
    "                        self.policy[x, y, a] = 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "                    else:\n",
    "                        self.policy[x, y, a] = self.epsilon / self.num_actions\n",
    "        return self.policy\n",
    "\n",
    "    def update_state_value(self):\n",
    "        \"\"\"\n",
    "        根据当前self.Q和self.policy计算状态值V(s)，并赋值给self.V\n",
    "        \"\"\"\n",
    "        for x in range(self.x_col):\n",
    "            for y in range(self.y_row):\n",
    "                self.V[x, y] = np.sum(self.policy[x, y] * self.Q[x, y])\n",
    "        return self.V\n",
    "\n",
    "    def update_QNet(self):\n",
    "        loss_avg = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            states, actions, rewards, next_states, next_actions, dones = self.buffer.sample(self.buffer.size())\n",
    "\n",
    "            # 转换为张量\n",
    "            states_actions = self.state_action_to_tensor(states, actions)\n",
    "            rewards_tensor = torch.tensor(rewards, dtype=torch.float).view(-1, 1)\n",
    "            next_states_actions = self.state_action_to_tensor(next_states, next_actions)\n",
    "            dones_tensor = torch.tensor(dones, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "            # 计算当前 Q 值\n",
    "            q_values = self.QNet(states_actions)\n",
    "\n",
    "            # 计算目标 Q 值\n",
    "            with torch.no_grad():\n",
    "                q_next_values = self.QNet(next_states_actions)\n",
    "                td_target = rewards_tensor + self.gamma * q_next_values * (1 - dones_tensor)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_fn(q_values, td_target)\n",
    "\n",
    "            # 反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "        return loss_avg/self.num_epochs\n",
    "\n",
    "    def run(self):\n",
    "        loss = 0\n",
    "        for episode in range(self.num_episodes):\n",
    "            # 清空缓冲区\n",
    "            self.buffer.clear()\n",
    "            # 生成1条episode并存入缓冲区\n",
    "            self.generate_episode()\n",
    "            # 进行多次小批量更新\n",
    "            loss += self.update_QNet()\n",
    "            # 更新Q值估计\n",
    "            self.update_action_value()\n",
    "            # 更新策略\n",
    "            self.update_policy()\n",
    "            # 更新状态值\n",
    "            self.update_state_value()\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode + 1} finished, loss: {loss/10:.4f}\")\n",
    "                loss = 0\n",
    "\n",
    "\n"
   ],
   "id": "c7af6b130bedd0de",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:35:52.589855Z",
     "start_time": "2026-01-10T09:34:51.234565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = GridWorld()\n",
    "env.reward_step = 0\n",
    "env.reward_target = 100\n",
    "\n",
    "agent = SarsaValue(env, epsilon=0.1, gamma=0.99, num_episodes=1000, episode_length=1000, batch_size=512, num_epochs=100, alpha=1e-3)\n",
    "agent.run()\n",
    "# agent.render_static()\n",
    "\n",
    "print(\"Final Policy:\")\n",
    "print(agent.get_policy())"
   ],
   "id": "6da056dc16dd0156",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 finished.\n",
      "Episode 20 finished.\n",
      "Episode 30 finished.\n",
      "Episode 40 finished.\n",
      "Episode 50 finished.\n",
      "Episode 60 finished.\n",
      "Episode 70 finished.\n",
      "Episode 80 finished.\n",
      "Episode 90 finished.\n",
      "Episode 100 finished.\n",
      "Final Policy:\n",
      "[[[0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]]\n",
      "\n",
      " [[0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.92 0.02 0.02 0.02]]\n",
      "\n",
      " [[0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.02 0.02 0.92 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.92 0.02 0.02 0.02]]\n",
      "\n",
      " [[0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.02 0.02 0.02 0.92]]\n",
      "\n",
      " [[0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.92 0.02 0.02 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.02 0.92 0.02 0.02]\n",
      "  [0.02 0.02 0.92 0.02 0.02]]\n",
      "\n",
      " [[0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.02 0.02 0.02 0.92 0.02]\n",
      "  [0.2  0.2  0.2  0.2  0.2 ]\n",
      "  [0.02 0.02 0.02 0.92 0.02]\n",
      "  [0.02 0.02 0.92 0.02 0.02]]\n",
      "\n",
      " [[0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.92 0.02 0.02 0.02 0.02]\n",
      "  [0.02 0.02 0.02 0.92 0.02]\n",
      "  [0.02 0.02 0.02 0.92 0.02]\n",
      "  [0.02 0.02 0.02 0.92 0.02]]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-10T09:34:37.531165Z",
     "start_time": "2026-01-10T09:34:37.524035Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c464b3eee4c560fb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
