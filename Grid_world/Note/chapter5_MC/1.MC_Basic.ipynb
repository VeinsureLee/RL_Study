{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "sys.argv = ['']\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.grid_world import GridWorld\n",
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, env, iterations, episodes_steps, episode_length, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Monte Carlo Basic Policy Iteration for a GridWorld environment.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.episodes_steps = episodes_steps\n",
    "        self.episode_length = episode_length\n",
    "\n",
    "        self.num_states = env.num_states\n",
    "        self.num_actions = len(env.action_space)\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "        self.V = np.zeros(self.num_states)\n",
    "        self.Q = np.zeros((self.num_states, self.num_actions))\n",
    "        self.policy = np.ones((self.num_states, self.num_actions)) / self.num_actions\n",
    "\n",
    "    def state2idx(self, state):\n",
    "        x, y = state  # state是(x,y)坐标\n",
    "        return y * self.env.env_size[0] + x\n",
    "\n",
    "    def idx2state(self, idx):\n",
    "        x = idx % self.env.env_size[0]\n",
    "        y = idx // self.env.env_size[0]\n",
    "        return x, y\n",
    "\n",
    "    def action2idx(self, action):\n",
    "        return self.action_space.index(action)\n",
    "\n",
    "    def idx2action(self, idx):\n",
    "        return self.action_space[idx]\n",
    "\n",
    "    def generate_episode(self, start_state_idx, start_action_idx):\n",
    "        \"\"\"\n",
    "        Generate an episode following current policy π.\n",
    "        Returns the list of (s,a,r) and total discounted return.\n",
    "        \"\"\"\n",
    "        s_idx = start_state_idx\n",
    "        a_idx = start_action_idx\n",
    "        s = self.idx2state(s_idx)\n",
    "        a = self.idx2action(a_idx)\n",
    "\n",
    "        episode_list = []\n",
    "        total_return = 0.0\n",
    "\n",
    "        for t in range(self.episode_length):\n",
    "            next_state, reward = self.env._get_next_state_and_reward(s, a)\n",
    "            episode_list.append((s_idx, a_idx, reward))\n",
    "            total_return += (self.gamma ** t) * reward\n",
    "\n",
    "            if next_state != self.env.target_state:\n",
    "                s_idx = self.state2idx(next_state)\n",
    "                s = next_state\n",
    "                a_idx = self.choose_action(s_idx)\n",
    "                a = self.idx2action(a_idx)\n",
    "\n",
    "        return episode_list, total_return\n",
    "\n",
    "    def choose_action(self, state_idx):\n",
    "        \"\"\"\n",
    "        Choose an action according to current policy (epsilon-soft)\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            max_value = np.max(self.policy[state_idx])\n",
    "            best_actions = np.where(self.policy[state_idx] == max_value)[0]\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def iteration(self):\n",
    "        \"\"\"\n",
    "        Run MC Basic Policy Iteration\n",
    "        \"\"\"\n",
    "        for it in range(self.iterations):\n",
    "            for s in range(self.num_states):\n",
    "                for a in range(self.num_actions):\n",
    "                    returns = []\n",
    "                    for ep in range(self.episodes_steps):\n",
    "                        _, r = self.generate_episode(s, a)\n",
    "                        returns.append(r)\n",
    "                    # Policy evaluation: average return over multiple episodes\n",
    "                    self.Q[s, a] = np.mean(returns)\n",
    "\n",
    "                # Policy improvement: greedy wrt Q\n",
    "                max_value = np.max(self.Q[s])\n",
    "                best_actions = np.where(self.Q[s] == max_value)[0]\n",
    "                chosen_action = np.random.choice(best_actions)\n",
    "\n",
    "                self.V[s] = self.Q[s, chosen_action]\n",
    "\n",
    "                for i in range(self.num_actions):\n",
    "                    self.policy[s][i] = 1.0 if i == chosen_action else 0.0\n",
    "            print(\"iteration %d done\" % it)\n",
    "\n",
    "    def render(self, precision=1):\n",
    "        \"\"\"\n",
    "        Render the GridWorld with the optimal value function and policy.\n",
    "        \"\"\"\n",
    "        self.env.render_static(values=self.V, policy=self.policy, precision=precision)\n"
   ],
   "id": "8319caeddf6c04d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = GridWorld()\n",
    "env.reward_target = 100\n",
    "env.reward_step = 0\n",
    "env.reward_forbidden = -1\n",
    "env.reset()\n",
    "agent = MonteCarlo(env, iterations=5, episodes_steps=5, episode_length=100, gamma=1, epsilon=0)\n",
    "agent.iteration()\n",
    "agent.render()"
   ],
   "id": "710d4f776ea10ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "看起来太过于容易陷入局部最优了，不知道这种真的能实现agent的路径规划吗，之后看吧",
   "id": "bdeac258af8755a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fuck, 是idx2state的xy写反了",
   "id": "4aa66e9fcadae2a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1b523b1c319e0f73",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
